# Kubernetes Demo

## Kubernetes Inro

### What is Kubernetes?

It's an open source container orchestration tool developed by google which helps you manage containerized applications on different deployment environments Physical, virtual, cloud or hybrid.



### What features does orchestration tools offer?

+ **High Availability** no downtime.


+ **Scalability** or high performance.


+ **Disaster Recovery** Backup and Restore.




### Kubernetes Components:

+ **POD**   : A layer at the top of the container. so, k8s communicate with this pod not the container.


+ **Service**: Permanent IP address. if the pod dies the service will stay.
	* **Internal Service.**
	* **External Service.**
	

+ **Ingress** : User end-point.


+ **Volume** : is used to make the data persist. and it can by local or cloud hard drive.


+ **ConfigMap**: used to store app configuration as DB URL. because if u used this configuration within the app through Environment variables or config file and something change u will need re-build.
but if u made the app use it from the ConfigMap file u can change it without any re-build.


+ **Secret**: Used to store Secret data like username and password.


+ **Deployment** : it's a layer at the top of pods and it's responsible about pod replicate and deployments on the nodes.


+ **StatefulSet** : it's like deployment but for database pods.



### Kubernetes Architecture:
![](./images/k8s-arch.png)

+ **Node** (Worker Machine): the note where you deploy the pods.


	* **Kubelet**: it interact with both node and container.and it starts pod with a container inside and assign node resources to it
	

	* **kube proxy**: mange the requests between pods.
	

	* **docker runtime**
	

+ **Master Node** (Control Panel): 
	* **Kube API Server**: it's the cluster gateway. it receive update requests and queries.
	

		- Acts like a gatekeeper for the authentication because it is the only way to cluster.
	* **Scheduler**: Deiced where to put the pod.
	

	* **Controller Manager**: detects cluster state changes. 
	

	* **etcd**: it the cluster brain, it has all cluster data. cluster changes get stored in the key value store.  
	

**You can have as many nodes as you want and at least two masters**


		

	 

### Minikube Installation


+ Apply all updates of existing packages of your system by executing the following apt commands,
```
$ sudo apt update -y
$ sudo apt upgrade -y
```



+ Once all the updates are installed then reboot your system once.
```
$ sudo reboot
```



+ Install the following minikube dependencies by running beneath command,
```
$ sudo apt install -y curl wget apt-transport-https
```



+ Use the following curl command to download latest minikube binary,
```
$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
```



+ Once the binary is downloaded, copy it to the path /usr/local/bin and set the executable permissions on it.
```
$ sudo install minikube-linux-amd64 /usr/local/bin/minikube
```



+ Verify the minikube version
```
$ minikube version
minikube version: v1.27.0
commit: 4243041b7a72319b9be7842a7d34b6767bbdac2b
```





### Start minikube

+ As we are already stated in the beginning that we would be using docker as base for minikue, so start the minikube with the docker driver, run
```
$ minikube start --driver=docker
```


+ In case you want to start minikube with customize resources and want installer to automatically select the driver then you can run following command,
```
$ minikube start --addons=ingress --cpus=2 --cni=flannel --install-addons=true --kubernetes-version=stable --memory=6g
```



+ Run below minikube command to check status,
```
$ minikube status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
pkumar@linuxtechi:~$
```



+ Run following kubectl command to verify the Kubernetes version, node status and cluster info.
```
$ kubectl cluster-info
$ kubectl get nodes
```

### Minikube Basics

*Accessing The K8s Dashboard*

Minikube implements the Kubernetes Dashboard out of the box. You can use the Kubernetes dashboard to monitor your clusterâ€™s health, or to deploy applications manually. If you deployed Minikube locally, you can access the dashboard by running the minikube dashboard command:
```
$ minikube dashboard
```

`ctrl+c` to exit.

 
 
 
 
 ### YAML Configuration file in Kubernetes
 
**Any configuration file has 3 parts:**
 - Metadata
 - Specifications
 - Status: this auto-generated by k8s to compare between current state and desired state.
 
 
 
 

 
In demo Project


### Demo Project

Done

### Install K8s with Kubeadm

































## Advanced Concepts:

### NAMESPACE  :

#### What is a Namespace?
it's a way to organize your deployments and u can think of it as a virtual cluster inside k8s cluster to use it for Organizing resources in namespaces.
```
$ kubectl get namespaces
```

+ There are four default namespaces
	* **kubernetes-dashboard:** it's in minicube only
	* **kube-system:** it's not for u to use. it used for system processes and master/kubectl processes.
	* **kube-public**: contain public information which anyone can access.
	* **default**: this is the namespace that u use by default.
	
#### Create a Namespace:
```
$ kubectl create namespace [NAMESPACE_NAME]
```


#### Namespace Use cases:

+ Group Resources In Namespaces:
![](./images/namespace1.png)
+ Conflicts: Many teams, same application:
![](./images/namespace2f.png)
![](./images/namespace2t.png)
+ Resource Sharing: Staging and Development:
![](./images/namespace3.png)
+ Resource Sharing: Blue/Green Deployment:
![](./images/namespace3b.png)
+ Access and Resources Limits on Namespaces:
![](./images/namespace4.png)


#### Characteristics of namespaces?

+ u can't use configMap file and secret file from another namespace. instead create them in each namesapce
![](./images/namespace-c1.png)
+ Access resources from another namespace:
![](./images/namespace-c2.png)
+ Components, which can't be created within a Namespace
	* Live globally in a cluster
	* you can't isolate them 
	* e.g. volume and node  
	
![](./images/namespace-c3.png)

#### Create component in a namespace

```
kubectl apply -f mysql-con.yaml --namespace=my-namespace
```


```
$ kubectl get configmap -n my-namespace
```

and u can do it in configMap  
![](./images/namespace-e1.png)


#### Change the active namespace with kubens!

Install Kubectx on ubuntu


### Kubernetes Ingress

Use a valid domain to access the service like '**https://myapp.com**'

#### External Service VS. Ingress:  

**External Service** Accessed by
**Service-IP:Port**  
ex. http://12.33.13.1:8000

**Ingress** is accessed by
**Domain-name**  
ex. https://my-app.com

![](./images/ivse-1.png)
![](./images/ivse-1.png)

#### Example YAML File: External Service

![](./images/ex-service.png)


#### Example YAML File: Ingress

![](./images/ex-Ingress.png)

#### Ingress and Internal Service configuration
![](./images/ingress-in-service.png)
![](./images/ingress-in-service2.png)

#### Configure Ingress
![](./images/ingress-1.png)

#### What is Ingress Controller ?
![](./images/ingress-2.png)

#### Install Ingress Controller in Minikube

```
$ minikube addons enable ingress
```

Automatically starts the K8s Nginx implementation of Ingress Controller  

```
$ kubectl get pod -n kube-system
```

#### Example: Create Ingress for kubernetes-dashboard

in minikube internal service and pod already exists.

```
$ kubectl get all -n kubernetes-dashboard
```

**dashboard-ingress.yml**   
![](./images/dashboard-ingress.png)

> map your /etc/hosts to dashboard.com


```
$ kubectl apply -f dashboard-ingress.yml
```

```
$ kubectl get ingress -n kubernetes-dashboard
```

#### Configure Default Backend in ingress

Add custom 404 page   
![](./images/default-b.png)


#### Multiple Paths
![](./images/m-paths.png)

#### Sub-domains
![](./images/sdomain.png)
![](./images/sdomain2.png)

#### TLS Certificate
![](./images/tls.png)
![](./images/tls2.png)

### Helm - Package Manager of K8s


#### What is Helm ?
it a package manager of K8s where u can find any templates that somebody did before to use it for your deployment.
 this is helm chart.
So, u just reuse files that someone made and save time and effort.  

To use search on it.  
+ Using CLI tool
```
$ helm search <KEYWORD>
```

+ Using Helm Hub  

**Helm Make us use deployment as a single application with all services and component (Installer)**  
  
  
  Helm works as:
  + a package manager.
  + Installer and uninstaller for apps.
  + Release manager.
  
**With Helm make us use apps like apps not just multiple kubernetes files.**  


#### Using Helm as Template Engine
 Instead of using multiple files for applications with the same configuration but just the name and image are different. u can use helm and use a single file with with different values.
 ![](./images/h1.png)
 U can use the values in two ways:
 + First, using values.yaml
 + Second, using ``` --set```  in the CLI   
 
 #### Helm Chart Structure
  ![](./images/h2.png)  
  ![](./images/h3.png)
  ![using default values](./images/h4.png)
 > This is very smart and useful then just using Kubectl.
 
#### Installing Helm on ubuntu:
+ Download your desired version
+ Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz)
+ Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm)

#### Helm Basic Commands
```
$ helm install wordpress			// install app with its all comps
$ helm update wordpress			// update your app
$ helm rollback wordpress		// return to the old version
$ helm uninstall wordpress		// remove the app
```
helm help  
```
$ helm --help					// faster than searching the internet
$ helm repo --help				// for sub commands
$ helm repo update --help		// for sub sub commands
$ helm search wordpress			// search on default repo
$ helm search hub wordpress		// search on artifact hub
$ helm repo list					// list all repos
$ helm repo update				// like 'apt update'
```

>it's more better than using kubectl to deploy each element. 

#### Helm Demo Project [Deploying Wordpress]
```
$ helm repo add bitnami https://charts.bitnami.com/bitnami
$ helm install my-release bitnami/wordpress
```
  
  
 ![deploying wordpress](./images/wp1.png)
 
 ```
 $ helm list				// show all my apps
 ```
 
 
 ```
 $ helm uninstall my-release
 ```
 
 
 
 
 
 
### Kubernetes Volumes

it's How to persist data in kubernetes.


Storage Types in K8s:

- Storage Class.
- Persistent Volume.
- Persistent Volume Claim.



Storage Requirements:

- Storage that doesn't depend on the pod lifecycle.
- Storage must be available on all nodes.
- Storage needs to survive even if cluster crashes.


#### Persistent Volume [PV]

- It's a virtual Volume mapped to Physical storage in/out side the cluster.

- a cluster resource.

- created via YAML file.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
	name: pv-name
spec:
	capacity:
		storage: 5Gi
	volumeMode: Filesystem
	accessModes:
		- ReadWriteOnce
	persistentVolumeReclaimPolicy: Recycle
	storageClassName: slow
	mountOptions:
		- hard
		- nfsvers=4.0
	nfs:
		path: /dir/path/on/ntfs/server
		server: nfs-server-ip-address
```


- PV live outside of the namespaces.
- accessible to the whole cluster.
- PV are resources that need to be there BEFORE using them in any POD.
- Application has to claim the Persistent Volume via Presistent Volume Claim [PVC]

#### Persistent Volume Claim

- is also created by a YAML configuration.

```
kind: PersostentVolumeClaim
apiVersion: v1
metadata:
	name: pvc-name
spec:
	storageClassName: manual
	volumeMode: Filesystem
	accessModes:
		- ReadWrite
	resources:
		requests:
			storage: 10Gi
```

- PVC request a storage with specific capacity and access type then whatever PV matches this critiria will be used.

- you need to use PVC in POD specs.

```
apiVersion: v1
kind: Pod
metadata:
	name: mypod
spec:
	containers:
		- name: myfrontent
		  image: nginx
		  volumeMounts:
		  - mountPath: "/var/www/html"
		    name: mypd
	volumes:
		- name: mypd
		  persistentVolumeClaim:
		  	claimName: pvc-name
```


- Claim must be in the same namespace with the POD.

- Volume is mounted into the POD.

- Volume is mounted into the container.


#### ConfigMap and Secret

- local volumes.
- not created via PV and PVC but managed by kubernetes itself.
- we use it when we use a configuration file for our POD or Certificate file.


How this works?

- Create ConfigMap and/or Secret componet.
- mount that into your pod/container.

```
apiVersion: v1
kind: Pod
metadata:
	name: mypod
spec:
	containers:
		- name: busybox-container
		  image: busybox
		  volumeMounts:
		  	- name: config-dir
		  	  mountPath: /etc/config
		  	  
	volumes:
		- name: config-dir
		  configMap:
		  	name: bb-configmap
```



Recap

- Volume is directory with some data.
- These volumes are accisible in containers in a pod.




#### Storage Class [SC]

SC provisions Persistent Volumes dynamically when PersistentVolume claim it.

- It created by a YAML configuration file too.
```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
	name: storage-class-name
provisioner: kubernetes.io/aws-ebs
parameters:
	type: io1
	iopsPerGB: "10"
	fsType: ext4
```

- 'provisioner' attribute tells K8s how to use this storage.
- each storage backend has its own provisioner.
- there are internal provisioner like 'kubernetes.io' and external provisioners.

- it's Requested by PersistentVolumeClaim.

```
kind: PersostentVolumeClaim
apiVersion: v1
metadata:
	name: pvc-name
spec:
	accessModes:
		- ReadWrite
	resources:
		requests:
			storage: 10Gi
	storageClassName: storage-class-name
```



1- Pod claims storage via PVC.

2- PVC requests storage from SC.

3- SC creates PV that meets the needs of the Claim.



SC has many types you can use NFS, AWS, Azure, and even Local storage.  

> Local Storage used with Minikube, Kind, and any other test environments. and it's simply use host storage into K8s Cluster.



To get available Storage Classes

```
$ kubectl get Storageclass 
```
 
Craete a PV
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-volume
  labels:
    type: local
spec:
  #we use local node storage here!
  #kubectl get storageclass
  storageClassName: hostpath
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```





### StatefulSet [sts]


It's like Deployment but for Stateful applications like databases or any application that stores data.

The different between deployment and statefulset:

-  cant be randomly addressed, deleted, or created at the same time.
-  each pod has sticky identity for each pod
	+  mysql-0 mysql-1 mysql-2
	
- Pods replica work as Master/Slave where only master can write.

![](./images/sts.png)

- all slaves syncs data updates from the master.

- Each pod has its own volume. they do not use the same physical Storage!
- When a new POD been added it's clone the data from the previous pod.
- There is Containus sycronization to keep all workers up to date.

- If you don't use a PV to persist the data. if all pods down at the same time the data will be lost.

- Pod state are stored on the PV. so, if the pod dead and replaced by another pod it will take the same state as the previous one. same ID,same service, same Service.

- Every Pod has its own identifier `$(statefulset_name)-$(ordinal)`

- Next Pod is only created, if previous us up and running!
- the deletion in reverse order, starting from the last one.

**2 Pod endpoints:**
1- Loadbalancer service same as Deployment.
2- individual service name for each pod  mysql-0.svc2 and this to allow pods to communicate with each others and if the pod die and replaced the other pod will up with the same pod state and service.

`${pod_name}.${governing_service_domain}`

**2 Characteristics:**
1- Predictable POD name `mysql-0`
2- Fixed individual DNS name `mysql-0.svc2`

when POD restarts ip address changes but name and endpoint stays same.




**Stateful applications not perfect for containerized environments**




### Services [svc]


What is a Service and when we need it?


- Each POD has its own IP address but it destroyed frequently!
- Service is a stable IP address, loadbalacer for all PODs replica


#### Services Types:

- ClusterIP Services
- NodePort Services
- Headless Services [None] 
- LoadBalancer Services

#### 1- ClusterIP service

- default type.
- Internal Service.



Service Communication: Selector

- it's how service know to which PODs should forward the request to and which port.
- the service knows its endpoint pods using the selector.

```
apiVersion: v1
kind: service
metadata:
	name: micro-one-service
spec:
	selector:
		app: micro-one
```


- Key value pairs for labels of pods.
```
labels:
	app: micro-one
```


- if more than one label pods must match all the selectors. means if the selector has to lables the will only add pods with these two labels not only one of them.


- K8s creates Endpoint object the same name as service to keeps track of which PODs are members/endpoints of the service.

```
$ kubectl get endpoints
```


**port vs targetPort**


`Port:` service port where u can specify any

`targetPort:` container port.

- ClusterIP service is also  used for communications between the pods.

![]()


**Multiport Services**
 
![]()



#### 2- Headless Service

- If the client want to communicate with 1 specific Pod directly.
- If Pods want to talk directly with specific Pod.


Use case: statful applications, like databases mysql, mongoDB, elasticsear.

- To get POD IP not the service make clusterIP: `None`.

```
spec:
	clusterIP: None
	selector:
		app: mongodb
	ports:
		- protocol: TCP
		  port: 27017
		  targetPort: 27017
```

Now it will return Pod IP address and client can use it to connect to the pod directly.



#### 3- NodePort Service

It's create a service that is accessable from a static port on each node.

```
spec:
	type: NodePort
	selector:
		app: micro-one
	ports:
		- protocol: TCP
		  port: 3200
		  targetPort: 3000
		  nodePort: 30008    # must be between [30000 - 32767]
```


NodePort --> ClusterIP port --> Pod port

ClusterIP Service is automatically created.



![NodePort on Multple nodes]()


- This is not secure because the client has access to the worker nodes directly.



#### 4- LoadBalancer Services

- LoadBalancer Service is an extension of NodePort Service.
- NodePort Service is an extension of clusterIP Service.
- LoadBalancer becomes accessible externally through cloud providers LoadBalancer.
- When we create a LoadBalancer Service a NodePort and CluterIP services are created automatically.

```
spec:
	type: LoadBalancer
	selector:
		app: micro
	ports:
	 - protocol: TCP
	   port: 3200	# ClusterIP Service port
	   targetPort: 	# Pod Port
	   nodePort: 30010
```

- The different between nodePort and Loadbalancer is the nodePort will be accessable only through the loadbalancer.










### K8s Operator

- used for Stateful applications to manage them and automate the hard work. eg update the images or if a pod die replace it with a new one with the latest configuration.


- it's Control loop mechanism.

- it uses CRD's [Custom Resources Definaitions] which are custom K8s component (extends K8s API).

- domain/app-specific knowledge --> {CRD's, StatefulSet, Configmap, Service} to automate entire lifecycle of the app it operates.


- K8s Manges complete lifecycle of Stateless apps.


- K8s cant automate the process natively for stateful apps. so, it uses its extention (operator) to manage these apps each one with its operator. eg. prometheus-operator, mysql-operator, postgres-operator.


- *Who creates these operators?*  the experts with this application. the expert team of mysql creates mysql operator.


#### Usage

- U can use it to deploy custom resources and work with it via `kubectl` like and k8s resource.

```
apiVersion:
cache.example.com/v1alpha1
kind: Memcahced
metadata:
	name: memcached-sample
spec:
	size: 3 # this is a custom field
status:
	nodes: [node_a, node_b, node_c]	# this also a custom field
```



#### How to build an operator

We can build it using one toolkit called opertator SDK and u can build the operator by:

- using helm chart and convert them to an operator.
- using ansible playbooks and convert them to an operator.
- and u can use Java-plugin or GO to create it from scratch.


#### Custom Field
 
as show in the above example we have fields that k8s doesn't know about it. but we need to let him know about them and store them in `etcd`. and this is done by operator SDK.




#### Control loop

is the intiligent of your operator. if you want to do backups, check, or any logic.


the Loop works as

Desired State --> current state --> reconciliation --> status --> Desired State

`reconciliation` asks what steps should we take to meet the desired state.







### Custom Resource Definition [CRD]

















## AWS Containerization Services:






### AWS Container Registry [ECR]

it's a registry for container's images.  
Pricing:
+ As a new Amazon ECR customer, you get 500 MB per month of storage for your private repositories for one year as part of the AWS Free Tier.
+ Both new and existing customers get 50 GB per month of always-free storage for their public repositories. You can anonymously (without using an AWS account) transfer 500 GB of data to the Internet from a public repository each month for free. If you sign up for an AWS account, or authenticate to Amazon ECR with an existing AWS account, you can transfer 5 TB of data to the Internet from a public repository each month for free. You also get unlimited bandwidth at no cost when transferring data from a public repository to AWS compute resources in any AWS Region.
+ Pricing details (beyond free tier limits): Storage is $0.10 per GB / month for data stored in private or public repositories.

  
**Steps to use it:**
+ login
```
$ aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/u5s8e1l6
```

+ Tag the image  
```
$ docker tag smartnotes:latest public.ecr.aws/u5s8e1l6/smartnotes:latest
```

+ Push
```
$ docker push public.ecr.aws/u5s8e1l6/smartnotes:latest
```


### EKS

